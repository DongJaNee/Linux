[root@headnode ~]# cat > job.slurm << 'EOF'
Submitted batch job 9
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
                 8    normal mpi_test  mpiuser PD       0:00      2 (Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions)
                 9    normal mpi_test     root PD       0:00      2 (ReqNodeNotAvail, UnavailableNodes:computenode)
cat: 'mpi_job_*.out': ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤



í˜„ì¬ Slurm ì‘ì—…ì´ ì‹¤í–‰ë˜ì§€ ì•Šê³  ëŒ€ê¸°(PENDING) ìƒíƒœì´ë©°, ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ë‚˜íƒ€ë‚˜ê³  ìˆìŠµë‹ˆë‹¤:

 í•´ê²° ë°©ë²•
1. compute node ìƒíƒœ í™•ì¸ (headnodeì—ì„œ ì‹¤í–‰)

scontrol show node computenode

ì¶œë ¥ ì˜ˆì‹œì—ì„œ State=DOWN, DRAIN, UNAVAILABLE ë“±ì˜ ìƒíƒœì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
2. compute nodeì—ì„œ slurmd ìƒíƒœ í™•ì¸

compute node(192.168.0.37)ì— ì ‘ì†í•œ ë’¤:

sudo systemctl status slurmd

    active (running) ìƒíƒœê°€ ì•„ë‹ˆë©´ ë‹¤ìŒì„ ì‹¤í–‰:

sudo systemctl restart slurmd
sudo systemctl enable slurmd

3. headnodeì™€ computenode ê°„ í†µì‹  í™•ì¸

headnodeì—ì„œ:

ping computenode
ssh computenode

ì—°ê²°ì´ ì•ˆ ë˜ë©´ /etc/hosts íŒŒì¼ì— ë…¸ë“œ ì´ë¦„ê³¼ IPê°€ ì œëŒ€ë¡œ ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

4. firewalld, SELinux í™•ì¸ (compute nodeì—ì„œ)

sudo systemctl stop firewalld
sudo setenforce 0

scontrol update NodeName=computenode State=RESUME

ë§ˆì§€ë§‰ í™•ì¸: ìŠ¬ëŸ¬ë¯¸ì—ì„œ í†µì‹ ì´ ê°€ëŠ¥í•œì§€ í™•ì¸

srun -N1 -n1 -w computenode hostname

ì´ ëª…ë ¹ì´ compute nodeì˜ í˜¸ìŠ¤íŠ¸ë„¤ì„ì„ ì¶œë ¥í•˜ë©´ ìŠ¬ëŸ¼ í†µì‹  OKì…ë‹ˆë‹¤.

ë…¸ë“œ ìƒíƒœê°€ IDLEë¡œ ë³µêµ¬ë˜ë©´ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì‘ì—… ë‹¤ì‹œ ì œì¶œí•˜ì„¸ìš”:

sbatch job.slurm
squeue


ë‚˜ì˜ ë§:
Submitted batch job 11
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
                11    normal mpi_test     root PD       0:00      2 (None)
Job started at 2025. 05. 16. (ê¸ˆ) 15:46:20 KST
Running on computenode
slurmstepd: error: execve(): /root/./mpi_hello: No such file or directory

 ./mpi_hello íŒŒì¼ì´ ì—†ìŒ ë•Œë¬¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.

slurmstepd: error: execve(): /root/./mpi_hello: No such file or directory

 í•´ê²° ë°©ë²•: ì‹¤í–‰ íŒŒì¼ ê³µìœ  ë˜ëŠ” ë³µì‚¬

ë°©ë²• 1: NFSë¡œ ê³µìœ 

ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì€ NFS(Network File System)ë¥¼ í†µí•´ mpi_helloê°€ ìœ„ì¹˜í•œ ë””ë ‰í„°ë¦¬ë¥¼ ëª¨ë“  ë…¸ë“œì—ì„œ ë™ì¼í•˜ê²Œ ë§ˆìš´íŠ¸í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ğŸ”¸ ë°©ë²• 2: ìˆ˜ë™ ë³µì‚¬

# headnodeì—ì„œ mpi_hello ì»´íŒŒì¼ ë˜ëŠ” í™•ì¸
mpicc -o mpi_hello mpi_hello.c

# ì»´í“¨íŠ¸ ë…¸ë“œë¡œ ë³µì‚¬
scp mpi_hello root@computenode:/root/

ì´í›„ ë‹¤ì‹œ Slurm ì‘ì—… ì œì¶œ:

sbatch job.slurm

ğŸ” ì¶”ê°€ íŒ

í˜„ì¬ job.slurmëŠ” /rootì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ, mpi_helloê°€ ê·¸ ìœ„ì¹˜ì— ë°˜ë“œì‹œ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ë” ì•ˆì „í•˜ê²Œ í•˜ë ¤ë©´ job.slurmì—ì„œ ì ˆëŒ€ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”:

srun /root/mpi_hello

no file/directory ./mpi_hello

âœ… 1ë‹¨ê³„: mpi_hello.c ì†ŒìŠ¤ ì½”ë“œ ë§Œë“¤ê¸° (ì—†ìœ¼ë©´)

cat > mpi_hello.c << 'EOF'
```
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(NULL, NULL);
    
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_processor_name(processor_name, &name_len);

    printf("Hello from processor %s, rank %d out of %d processors\n",
           processor_name, world_rank, world_size);

    MPI_Finalize();
    return 0;
}
EOF
```

âœ… 2ë‹¨ê³„: ì»´íŒŒì¼

module load mpi/openmpi-x86_64
mpicc -o mpi_hello mpi_hello.c

âœ… 3ë‹¨ê³„: Slurm ì‘ì—… ë””ë ‰í„°ë¦¬ì— mpi_hello íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸

ls -l ./mpi_hello

âœ… 4ë‹¨ê³„: compute nodeë¡œ íŒŒì¼ ë³µì‚¬ (í…ŒìŠ¤íŠ¸ìš©)

scp mpi_hello root@computenode:/root/

âœ… 5ë‹¨ê³„: Slurm ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì‹¤í–‰ ê²½ë¡œ ìˆ˜ì •

í˜„ì¬ job.slurmì—ì„œ ìƒëŒ€ ê²½ë¡œ ./mpi_helloëŠ” ê° ë…¸ë“œì˜ ì‘ì—… ë””ë ‰í„°ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì°¾ê¸° ë•Œë¬¸ì— ì ˆëŒ€ ê²½ë¡œë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤:

cat > job.slurm << 'EOF'
```
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"

# ì ˆëŒ€ ê²½ë¡œë¡œ ì‹¤í–‰
srun /root/mpi_hello

echo "Job completed at $(date)"
EOF
```

âœ… 6ë‹¨ê³„: Slurm ì‘ì—… ì¬ì œì¶œ

sbatch job.slurm

ë‚˜ì˜ ë§:
slurmstepd: error: execve(): /root/./mpi_hello: No such file or directory

âœ… í•´ê²° ë°©ë²• ìš”ì•½
1. NFS ê³µìœ  ë””ë ‰í„°ë¦¬ì— ì‹¤í–‰ íŒŒì¼ ì˜®ê¸°ê¸° (head nodeì—ì„œ)

mkdir -p /home/mpiuser/shared
mv ~/mpi_hello /home/mpiuser/shared/
chmod +x /home/mpiuser/shared/mpi_hello

2. compute nodeì—ì„œ ë§ˆìš´íŠ¸ í™•ì¸

mount -t nfs 192.168.0.44:/home/mpiuser/shared /home/mpiuser/shared
ls /home/mpiuser/shared/mpi_hello

ì •ìƒì ìœ¼ë¡œ ë³´ì´ë©´ OKì…ë‹ˆë‹¤.

3. Slurm job ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • (job.slurm)
```
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=/home/mpiuser/shared/mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"

# ì‹¤í–‰ ê²½ë¡œ ìˆ˜ì •
srun /home/mpiuser/shared/mpi_hello

echo "Job completed at $(date)"
```
4. Slurm job ì œì¶œ

cd /home/mpiuser/shared
sbatch job.slurm

5. ê²°ê³¼ í™•ì¸

cat /home/mpiuser/shared/mpi_job_*.out


[root@computenode shared]# sbatch job.slurm
sbatch: error: Unable to open file job.slurm

ì´ ì—ëŸ¬ëŠ” Slurmì´ job.slurm íŒŒì¼ì„ í˜„ì¬ ë””ë ‰í„°ë¦¬ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤.

sbatch: error: Unable to open file job.slurm

âœ… í™•ì¸ ë° í•´ê²° ë°©ë²•
1. í˜„ì¬ ìœ„ì¹˜ í™•ì¸

pwd
ls -l

job.slurm íŒŒì¼ì´ /home/mpiuser/shared ë””ë ‰í„°ë¦¬ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
2. ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ Head Nodeì—ì„œ ë§Œë“¤ì–´ì£¼ì„¸ìš”

Head Nodeì—ì„œ ë‹¤ìŒì²˜ëŸ¼ job.slurm íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”:

cd /home/mpiuser/shared

cat > job.slurm << 'EOF'
```
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=/home/mpiuser/shared/mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"

srun /home/mpiuser/shared/mpi_hello

echo "Job completed at $(date)"
EOF
```

ê·¸ë¦¬ê³  mpi_hello ì‹¤í–‰ íŒŒì¼ë„ ê°™ì€ ìœ„ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

ls -l /home/mpiuser/shared/mpi_hello
chmod +x /home/mpiuser/shared/mpi_hello

3. ë‹¤ì‹œ compute nodeì—ì„œ ì‹¤í–‰

cd /home/mpiuser/shared
sbatch job.slurm


ë‚˜ì˜ ë§:
[root@computenode shared]# ls -l
í•©ê³„ 20
-rwxr-xr-x. 1 root root 17728  5ì›” 16 15:50 mpi_hello
ChatGPTì˜ ë§:

âœ… í•´ê²° ìˆœì„œ
1. ê¶Œí•œ ë° ì†Œìœ ì ë³€ê²½ (head node ë˜ëŠ” compute nodeì—ì„œ)

chown mpiuser:mpiuser /home/mpiuser/shared/mpi_hello

ë˜ëŠ” ì „ì²´ ë””ë ‰í„°ë¦¬ì— ëŒ€í•´:

chown -R mpiuser:mpiuser /home/mpiuser/shared

2. Slurm ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ë„ /home/mpiuser/sharedì— ë§Œë“¤ê¸°

ì•„ì§ job.slurmì´ ì—†ë‹¤ë©´ head nodeì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë§Œë“œì„¸ìš”:

cd /home/mpiuser/shared

cat > job.slurm << 'EOF'
```
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=/home/mpiuser/shared/mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"

srun /home/mpiuser/shared/mpi_hello

echo "Job completed at $(date)"
EOF
```

3. Slurm ì‘ì—… ì œì¶œ

cd /home/mpiuser/shared
sbatch job.slurm

4. ê²°ê³¼ í™•ì¸

cat mpi_job_*.out

ë‚˜ì˜ ë§:
[root@headnode ~]# pwd
/root
[root@headnode ~]# ls -l
í•©ê³„ 28
-rw-------. 1 root root 1383  5ì›” 13 15:44 anaconda-ks.cfg
-rw-r--r--. 1 root root   42  5ì›” 15 14:14 hostfile
-rw-r--r--. 1 root root  321  5ì›” 16 16:06 job.slurm
-rw-r--r--. 1 root root  542  5ì›” 15 13:48 mpi_hello.c
drwxr-xr-x. 2 root root   58  5ì›” 16 14:35 mpi_test
-rw-r--r--. 1 root root   12  1ì›” 28 09:17 nodes
-rwxr-xr-x. 1 root root   43  5ì›” 15 13:47 openmpi.sh
-rw-r--r--. 1 root root  155  5ì›” 15 15:58 test_job.sh

â— ë¬¸ì œ ìš”ì•½
    Slurm ì‘ì—…ì—ì„œëŠ” **ëª¨ë“  ë…¸ë“œ(head/compute)**ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ ìœ„ì¹˜ì— ì‹¤í–‰ íŒŒì¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

    í˜„ì¬ /rootëŠ” NFS ê³µìœ ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, compute nodeì—ì„œëŠ” /root/mpi_hello íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

    ë”°ë¼ì„œ ê³µìœ  ë””ë ‰í„°ë¦¬(/home/mpiuser/shared)ë¡œ ì´ë™ì´ í•„ìš”í•©ë‹ˆë‹¤.

âœ… í•´ê²° ì ˆì°¨
1. ê³µìœ  ë””ë ‰í„°ë¦¬ë¡œ íŒŒì¼ ë³µì‚¬ (headnodeì—ì„œ ì‹¤í–‰)

mkdir -p /home/mpiuser/shared
cp ~/mpi_hello /home/mpiuser/shared/
cp ~/job.slurm /home/mpiuser/shared/
chown mpiuser:mpiuser /home/mpiuser/shared/*
chmod +x /home/mpiuser/shared/mpi_hello

2. compute nodeì—ì„œ ê³µìœ  ë””ë ‰í„°ë¦¬ í™•ì¸

ls /home/mpiuser/shared

ì—¬ê¸°ì„œ mpi_helloì™€ job.slurmì´ ë³´ì´ë©´ OKì…ë‹ˆë‹¤.
3. Slurm ì‘ì—… ì œì¶œ (head ë˜ëŠ” compute nodeì—ì„œ)

cd /home/mpiuser/shared
sbatch job.slurm

4. ì‘ì—… ì™„ë£Œ í›„ ê²°ê³¼ í™•ì¸

cat /home/mpiuser/shared/mpi_job_*.out

ë‚˜ì˜ ë§:
[root@headnode ~]# cp ~/mpi_hello /home/mpiuser/shared
cp: cannot stat '/root/mpi_hello': ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤

ì—ëŸ¬ ë©”ì‹œì§€ì— ë”°ë¥´ë©´, /root/mpi_hello íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì˜ëª»ëœ ê²½ë¡œë¥¼ ì°¸ì¡°í•˜ê³  ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. /root/mpi_hello íŒŒì¼ì„ mpi_hello.c íŒŒì¼ì„ ì»´íŒŒì¼í•˜ì—¬ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

âœ… í•´ê²° ì ˆì°¨
1. mpi_hello.c ì»´íŒŒì¼í•˜ê¸°

ë¨¼ì €, mpi_hello.c íŒŒì¼ì„ ì»´íŒŒì¼í•˜ì—¬ mpi_hello ì‹¤í–‰ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

mpicc -o /root/mpi_hello /root/mpi_hello.c

ìœ„ ëª…ë ¹ì–´ëŠ” /root/mpi_hello.c íŒŒì¼ì„ ì»´íŒŒì¼í•˜ì—¬ /root/mpi_hello ì‹¤í–‰ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
2. ì‹¤í–‰ íŒŒì¼ì„ ê³µìœ  ë””ë ‰í„°ë¦¬ë¡œ ì´ë™

ì´ì œ ì»´íŒŒì¼ëœ mpi_hello íŒŒì¼ì„ /home/mpiuser/sharedë¡œ ë³µì‚¬í•©ë‹ˆë‹¤:

cp /root/mpi_hello /home/mpiuser/shared/
chown mpiuser:mpiuser /home/mpiuser/shared/mpi_hello
chmod +x /home/mpiuser/shared/mpi_hello

3. Slurm ì‘ì—… ì œì¶œ

ì´ì œ /home/mpiuser/shared/ ë””ë ‰í„°ë¦¬ì—ì„œ job.slurmì„ ì œì¶œí•©ë‹ˆë‹¤:

cd /home/mpiuser/shared
sbatch job.slurm

4. ê²°ê³¼ í™•ì¸

cat /home/mpiuser/shared/mpi_job_*.out

ë‚˜ì˜ ë§:
2. [root@headnode ~]# cp /root/mpi_hello /home/mpiuser/shared/
cp: cannot stat '/root/mpi_hello': ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤
ChatGPTì˜ ë§:

í˜„ì¬ /root/mpi_hello íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ëŠ” mpi_hello.c íŒŒì¼ì„ ì»´íŒŒì¼í•˜ì—¬ mpi_hello ì‹¤í–‰ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•Šì•„ì„œ ë°œìƒí•œ ë¬¸ì œì…ë‹ˆë‹¤.
âœ… í•´ê²° ë°©ë²•

    mpi_hello.c íŒŒì¼ ì»´íŒŒì¼

    ë¨¼ì €, /root/mpi_hello.c íŒŒì¼ì„ ì»´íŒŒì¼í•˜ì—¬ mpi_hello ì‹¤í–‰ íŒŒì¼ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

    mpicc -o /root/mpi_hello /root/mpi_hello.c

    ì»´íŒŒì¼ì´ ì™„ë£Œë˜ë©´ /root/mpi_hello ì‹¤í–‰ íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.

    mpi_hello íŒŒì¼ì„ ê³µìœ  ë””ë ‰í„°ë¦¬ë¡œ ë³µì‚¬

    ì»´íŒŒì¼ í›„, mpi_hello íŒŒì¼ì„ **ê³µìœ  ë””ë ‰í„°ë¦¬(/home/mpiuser/shared)**ë¡œ ì´ë™í•©ë‹ˆë‹¤.

    cp /root/mpi_hello /home/mpiuser/shared/
    chown mpiuser:mpiuser /home/mpiuser/shared/mpi_hello
    chmod +x /home/mpiuser/shared/mpi_hello

    Slurm ì‘ì—… ì œì¶œ

    ì´ì œ /home/mpiuser/shared/ ë””ë ‰í„°ë¦¬ë¡œ ì´ë™í•˜ì—¬ job.slurmì„ ì œì¶œí•©ë‹ˆë‹¤.

    cd /home/mpiuser/shared
    sbatch job.slurm

    ê²°ê³¼ í™•ì¸

    ì‘ì—…ì´ ì™„ë£Œë˜ë©´, ê²°ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:

    cat /home/mpiuser/shared/mpi_job_*.out

size_tëŠ” stddef.h í—¤ë” íŒŒì¼ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ë ¤ë©´ ì½”ë“œ ìƒë‹¨ì— #include <stddef.h>ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆì‹œ ì½”ë“œ:

#include <stddef.h>  // size_t ì •ì˜

ì„¸ ë²ˆì§¸ ì˜¤ë¥˜ (fatal error: mpi.h: No such file or directory)

    MPI ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸: ë¨¼ì € MPIê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. mpicc ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ ë³´ì„¸ìš”:

mpicc --version


ì»´íŒŒì¼ ì‹œ ê²½ë¡œ ì„¤ì •: ì»´íŒŒì¼í•  ë•Œ mpi.h íŒŒì¼ì´ ìœ„ì¹˜í•œ ê²½ë¡œë¥¼ ì•Œë ¤ì¤˜ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, mpicc ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•  ë•Œ -I ì˜µì…˜ìœ¼ë¡œ MPI í—¤ë” íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

        mpicc -I /path/to/mpi/include -o my_program my_program.c


[root@headnode ~]# mpicc -I /path/to/mpi/include -o my_program my_program.c
cc1: fatal error: my_program.c: ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤
compilation terminated.

nano my_program.c
```
    #include <mpi.h>
    #include <stdio.h>

    int main(int argc, char* argv[]) {
        MPI_Init(&argc, &argv);
        int rank;
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);
        printf("Hello, World! I am process %d\n", rank);
        MPI_Finalize();
        return 0;
    }
```

mpicc -I /path/to/mpi/include -o my_program my_program.c

ì˜ˆë¥¼ ë“¤ì–´, OpenMPIê°€ /usr/include/mpi/ì— ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´, ì•„ë˜ì™€ ê°™ì´ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

mpicc -I /usr/include/mpi/ -o my_program my_program.c

ì»´íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ëë‚¬ë‹¤ë©´ my_program ì‹¤í–‰ íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤. ls ëª…ë ¹ì–´ë¡œ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”:

ls -l my_program

íŒŒì¼ì´ ìƒì„±ë˜ì—ˆë‹¤ë©´, ì´ì œ ì‹¤í–‰í•  ì¤€ë¹„ê°€ ëœ ê²ƒì…ë‹ˆë‹¤.

í”„ë¡œê·¸ë¨ ì‹¤í–‰
MPI í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•˜ë ¤ë©´, mpirun ë˜ëŠ” mpiexec ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 4ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì…ë ¥í•©ë‹ˆë‹¤:

mpirun -np 4 ./my_program

ë˜ëŠ”

    mpiexec -np 4 ./my_program

    ì´ ëª…ë ¹ì–´ëŠ” 4ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì¶œë ¥ì€ ê° í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ë˜ëŠ” ìˆœì„œëŒ€ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.

ìœ„ ë‹¨ê³„ë¥¼ ë”°ë¼ ì§„í–‰í•˜ë©´ MPI í”„ë¡œê·¸ë¨ì„ ì»´íŒŒì¼í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë‚˜ì˜ ë§:
[root@headnode ~]# mpirun -np 4 ./my_program
--------------------------------------------------------------------------
mpirun has detected an attempt to run as root.

Running as root is *strongly* discouraged as any mistake (e.g., in
defining TMPDIR) or bug can result in catastrophic damage to the OS
file system, leaving your system in an unusable state.

We strongly suggest that you run mpirun as a non-root user.

You can override this protection by adding the --allow-run-as-root option
to the cmd line or by setting two environment variables in the following way:
the variable OMPI_ALLOW_RUN_AS_ROOT=1 to indicate the desire to override this
protection, and OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1 to confirm the choice and
add one more layer of certainty that you want to do so.
We reiterate our advice against doing so - please proceed at your own risk.
--------------------------------------------------------------------------

MPIëŠ” ë³´ì•ˆì„ ì´ìœ ë¡œ root ì‚¬ìš©ìë¡œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë©”ì‹œì§€ì—ì„œ ì„¤ëª…ëœ ëŒ€ë¡œ, mpirunì„ root ì‚¬ìš©ìë¡œ ì‹¤í–‰í•˜ë ¤ê³  í•  ë•Œ ê²½ê³ ê°€ ë°œìƒí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ë¥¼ ë¬´ì‹œí•˜ê³  ì‹¤í–‰í•˜ë ¤ë©´ --allow-run-as-root ì˜µì…˜ì„ ì¶”ê°€í•˜ê±°ë‚˜, í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
ë°©ë²• 1: --allow-run-as-root ì˜µì…˜ ì‚¬ìš©

mpirun --allow-run-as-root -np 4 ./my_program

ë°©ë²• 2: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

export OMPI_ALLOW_RUN_AS_ROOT=1
export OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
mpirun -np 4 ./my_program

ë‚˜ì˜ ë§:
Submitted batch job 11
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
                11    normal mpi_test     root PD       0:00      2 (None)
Job started at 2025. 05. 16. (ê¸ˆ) 15:46:20 KST
Running on computenode
slurmstepd: error: execve(): /root/./mpi_hello: No such file or directory

ğŸ”¸ ë°©ë²• 2: ìˆ˜ë™ ë³µì‚¬

# headnodeì—ì„œ mpi_hello ì»´íŒŒì¼ ë˜ëŠ” í™•ì¸
mpicc -o mpi_hello mpi_hello.c

# ì»´í“¨íŠ¸ ë…¸ë“œë¡œ ë³µì‚¬
scp mpi_hello root@computenode:/root/

ì´í›„ ë‹¤ì‹œ Slurm ì‘ì—… ì œì¶œ:

sbatch job.slurm
ë‚˜ì˜ ë§: no file/directory ./mpi_hello

âœ… 1ë‹¨ê³„: mpi_hello.c ì†ŒìŠ¤ ì½”ë“œ ë§Œë“¤ê¸° (ì—†ìœ¼ë©´)


cat > mpi_hello.c << 'EOF'
```
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(NULL, NULL);
    
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_processor_name(processor_name, &name_len);

    printf("Hello from processor %s, rank %d out of %d processors\n",
           processor_name, world_rank, world_size);

    MPI_Finalize();
    return 0;
}
EOF
```

âœ… 2ë‹¨ê³„: ì»´íŒŒì¼

module load mpi/openmpi-x86_64
mpicc -o mpi_hello mpi_hello.c

âœ… 3ë‹¨ê³„: Slurm ì‘ì—… ë””ë ‰í„°ë¦¬ì— mpi_hello íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸

ls -l ./mpi_hello

âœ… 4ë‹¨ê³„: compute nodeë¡œ íŒŒì¼ ë³µì‚¬ (í…ŒìŠ¤íŠ¸ìš©)

scp mpi_hello root@computenode:/root/

âœ… 5ë‹¨ê³„: Slurm ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì‹¤í–‰ ê²½ë¡œ ìˆ˜ì •

cat > job.slurm << 'EOF'
```
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"

# ì ˆëŒ€ ê²½ë¡œë¡œ ì‹¤í–‰
srun /root/mpi_hello

echo "Job completed at $(date)"
EOF
```
âœ… 6ë‹¨ê³„: Slurm ì‘ì—… ì¬ì œì¶œ

sbatch job.slurm

ë‚˜ì˜ ë§:
slurmstepd: error: execve(): /root/./mpi_hello: No such file or directory

ì¢‹ìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ ë³´ë©´ mpi_hello ì‹¤í–‰ íŒŒì¼ì„ ì—¬ì „íˆ **/root/./mpi_hello** ê²½ë¡œì—ì„œ ì°¾ê³  ìˆì–´ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤:

ì¦‰, Slurmì´ mpi_hello ì‹¤í–‰íŒŒì¼ì„ head nodeì™€ compute nodeì—ì„œ ë™ì¼í•œ ê²½ë¡œë¡œ ì°¸ì¡°í•˜ë ¤ê³  ì‹œë„í–ˆì§€ë§Œ, í•´ë‹¹ ìœ„ì¹˜ì— íŒŒì¼ì´ ì—†ì–´ì„œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.
âœ… í•´ê²° ë°©ë²• ìš”ì•½

1. NFS ê³µìœ  ë””ë ‰í„°ë¦¬ì— ì‹¤í–‰ íŒŒì¼ ì˜®ê¸°ê¸° (head nodeì—ì„œ)

mkdir -p /home/mpiuser/shared
mv ~/mpi_hello /home/mpiuser/shared/
chmod +x /home/mpiuser/shared/mpi_hello

2. compute nodeì—ì„œ ë§ˆìš´íŠ¸ í™•ì¸

mount -t nfs 192.168.0.44:/home/mpiuser/shared /home/mpiuser/shared
ls /home/mpiuser/shared/mpi_hello

ì •ìƒì ìœ¼ë¡œ ë³´ì´ë©´ OKì…ë‹ˆë‹¤.
3. Slurm job ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • (job.slurm)
```
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=/home/mpiuser/shared/mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"
```

# ì‹¤í–‰ ê²½ë¡œ ìˆ˜ì •
srun /home/mpiuser/shared/mpi_hello

echo "Job completed at $(date)"

4. Slurm job ì œì¶œ

cd /home/mpiuser/shared
sbatch job.slurm

5. ê²°ê³¼ í™•ì¸

cat /home/mpiuser/shared/mpi_job_*.out

ë‚˜ì˜ ë§:
[root@computenode shared]# sbatch job.slurm
sbatch: error: Unable to open file job.slurm

ì´ ì—ëŸ¬ëŠ” Slurmì´ job.slurm íŒŒì¼ì„ í˜„ì¬ ë””ë ‰í„°ë¦¬ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤.

âœ… í™•ì¸ ë° í•´ê²° ë°©ë²•
1. í˜„ì¬ ìœ„ì¹˜ í™•ì¸

pwd
ls -l

job.slurm íŒŒì¼ì´ /home/mpiuser/shared ë””ë ‰í„°ë¦¬ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

2. ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ Head Nodeì—ì„œ ë§Œë“¤ì–´ì£¼ì„¸ìš”

Head Nodeì—ì„œ ë‹¤ìŒì²˜ëŸ¼ job.slurm íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”:

cd /home/mpiuser/shared

cat > job.slurm << 'EOF'
```
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=/home/mpiuser/shared/mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"

srun /home/mpiuser/shared/mpi_hello

echo "Job completed at $(date)"
EOF
```
ê·¸ë¦¬ê³  mpi_hello ì‹¤í–‰ íŒŒì¼ë„ ê°™ì€ ìœ„ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

ls -l /home/mpiuser/shared/mpi_hello
chmod +x /home/mpiuser/shared/mpi_hello


3. ë‹¤ì‹œ compute nodeì—ì„œ ì‹¤í–‰

cd /home/mpiuser/shared
sbatch job.slurm

ë‚˜ì˜ ë§:
[root@computenode shared]# ls -l
í•©ê³„ 20
-rwxr-xr-x. 1 root root 17728  5ì›” 16 15:50 mpi_hello

âœ… í•´ê²° ìˆœì„œ
1. ê¶Œí•œ ë° ì†Œìœ ì ë³€ê²½ (head node ë˜ëŠ” compute nodeì—ì„œ)

chown mpiuser:mpiuser /home/mpiuser/shared/mpi_hello

ë˜ëŠ” ì „ì²´ ë””ë ‰í„°ë¦¬ì— ëŒ€í•´:

chown -R mpiuser:mpiuser /home/mpiuser/shared

2. Slurm ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ë„ /home/mpiuser/sharedì— ë§Œë“¤ê¸°

ì•„ì§ job.slurmì´ ì—†ë‹¤ë©´ head nodeì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë§Œë“œì„¸ìš”:

cd /home/mpiuser/shared

cat > job.slurm << 'EOF'
```
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=/home/mpiuser/shared/mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"

srun /home/mpiuser/shared/mpi_hello

echo "Job completed at $(date)"
EOF
```
3. Slurm ì‘ì—… ì œì¶œ

cd /home/mpiuser/shared
sbatch job.slurm

4. ê²°ê³¼ í™•ì¸

cat mpi_job_*.out

ë‚˜ì˜ ë§:
[root@headnode ~]# pwd
/root
[root@headnode ~]# ls -l
í•©ê³„ 28
-rw-------. 1 root root 1383  5ì›” 13 15:44 anaconda-ks.cfg
-rw-r--r--. 1 root root   42  5ì›” 15 14:14 hostfile

ì¢‹ìŠµë‹ˆë‹¤. í˜„ì¬ job.slurm, mpi_hello.c ë“±ì´ ëª¨ë‘ /root ë””ë ‰í„°ë¦¬ì— ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ Slurm ì‘ì—…ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

âœ… í•´ê²° ì ˆì°¨
1. ê³µìœ  ë””ë ‰í„°ë¦¬ë¡œ íŒŒì¼ ë³µì‚¬ (headnodeì—ì„œ ì‹¤í–‰)

mkdir -p /home/mpiuser/shared
cp ~/mpi_hello /home/mpiuser/shared/
cp ~/job.slurm /home/mpiuser/shared/
chown mpiuser:mpiuser /home/mpiuser/shared/*
chmod +x /home/mpiuser/shared/mpi_hello

2. compute nodeì—ì„œ ê³µìœ  ë””ë ‰í„°ë¦¬ í™•ì¸

ls /home/mpiuser/shared

ì—¬ê¸°ì„œ mpi_helloì™€ job.slurmì´ ë³´ì´ë©´ OKì…ë‹ˆë‹¤.
3. Slurm ì‘ì—… ì œì¶œ (head ë˜ëŠ” compute nodeì—ì„œ)

cd /home/mpiuser/shared
sbatch job.slurm

4. ì‘ì—… ì™„ë£Œ í›„ ê²°ê³¼ í™•ì¸

cat /home/mpiuser/shared/mpi_job_*.out

ì»´íŒŒì¼ : mpicc -o mpi_hello mpi_hello.c

ê·¸ í›„ /home/mpiuser/sharedë¡œ ë‹¤ì‹œ ë³µì‚¬í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.

ë‚˜ì˜ ë§:
[root@headnode ~]# cp ~/mpi_hello /home/mpiuser/shared
cp: cannot stat '/root/mpi_hello': ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤

âœ… í•´ê²° ì ˆì°¨
1. mpi_hello.c ì»´íŒŒì¼í•˜ê¸°

mpicc -o /root/mpi_hello /root/mpi_hello.c

2. ì‹¤í–‰ íŒŒì¼ì„ ê³µìœ  ë””ë ‰í„°ë¦¬ë¡œ ì´ë™

cp /root/mpi_hello /home/mpiuser/shared/
chown mpiuser:mpiuser /home/mpiuser/shared/mpi_hello
chmod +x /home/mpiuser/shared/mpi_hello

3. Slurm ì‘ì—… ì œì¶œ

cd /home/mpiuser/shared
sbatch job.slurm

4. ê²°ê³¼ í™•ì¸

cat /home/mpiuser/shared/mpi_job_*.out

ë‚˜ì˜ ë§:
[root@headnode ~]# cp /root/mpi_hello /home/mpiuser/shared/
cp: cannot stat '/root/mpi_hello': ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤

âœ… í•´ê²° ë°©ë²•

    cp /root/mpi_hello /home/mpiuser/shared/
    chown mpiuser:mpiuser /home/mpiuser/shared/mpi_hello
    chmod +x /home/mpiuser/shared/mpi_hello

    cd /home/mpiuser/shared
    sbatch job.slurm

    cat /home/mpiuser/shared/mpi_job_*.out

ë‚˜ì˜ ë§:
[root@headnode ~]# mpicc -I /path/to/mpi/include -o my_program my_program.c
[root@headnode ~]# ls -l my_program
-rwxr-xr-x. 1 root root 17624  5ì›” 16 16:16 my_program
[root@headnode ~]# mpirun -np 4 ./my_program
--------------------------------------------------------------------------
mpirun has detected an attempt to run as root.

Running as root is *strongly* discouraged as any mistake (e.g., in
defining TMPDIR) or bug can result in catastrophic damage to the OS
file system, leaving your system in an unusable state.
--------------------------------------------------------------------------

ë°©ë²• 1: --allow-run-as-root ì˜µì…˜ ì‚¬ìš©

mpirun --allow-run-as-root -np 4 ./my_program

ë°©ë²• 2: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```
export OMPI_ALLOW_RUN_AS_ROOT=1
export OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
mpirun -np 4 ./my_program
```

ìƒˆë¡œìš´ ì‚¬ìš©ì ìƒì„±: adduser mpiuser

í•´ë‹¹ ì‚¬ìš©ìë¡œ ë¡œê·¸ì¸: su - mpiuser

MPI í”„ë¡œê·¸ë¨ì„ ë‹¤ì‹œ ì‹¤í–‰: mpirun -np 4 ./my_program

ë‚˜ì˜ ë§:
[root@computenode ~]# ls mpi_job_*.out
mpi_job_11.out  mpi_job_13.out  mpi_job_9.out
[root@computenode ~]# cat mpi_job_11.out
Job started at 2025. 05. 16. (ê¸ˆ) 15:47:20 KST
Running on computenode
slurmstepd: error: execve(): /root/./mpi_hello: No such file or directory
slurmstepd: error: execve(): /root/./mpi_hello: No such file or directory
srun: error: computenode: tasks 0-1: Exited with exit code 2
slurmstepd: error: execve(): /root/./mpi_hello: No such file or directory
srun: error: headnode: tasks 2-3: Exited with exit code 2
Job completed at 2025. 05. 16. (ê¸ˆ) 15:47:20 KST
[root@computenode ~]# 

âœ… í•´ê²° ë°©ë²• ìš”ì•½
1. NFS ê³µìœ  ë””ë ‰í„°ë¦¬ì— ì‹¤í–‰ íŒŒì¼ ì˜®ê¸°ê¸° (head nodeì—ì„œ)

mkdir -p /home/mpiuser/shared
mv ~/mpi_hello /home/mpiuser/shared/
chmod +x /home/mpiuser/shared/mpi_hello

2. compute nodeì—ì„œ ë§ˆìš´íŠ¸ í™•ì¸

mount -t nfs 192.168.0.44:/home/mpiuser/shared /home/mpiuser/shared
ls /home/mpiuser/shared/mpi_hello

3. Slurm job ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • (job.slurm)
```
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=/home/mpiuser/shared/mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"

# ì‹¤í–‰ ê²½ë¡œ ìˆ˜ì •
srun /home/mpiuser/shared/mpi_hello

echo "Job completed at $(date)"
```
4. Slurm job ì œì¶œ

cd /home/mpiuser/shared
sbatch job.slurm

5. ê²°ê³¼ í™•ì¸

cat /home/mpiuser/shared/mpi_job_*.out

ë‚˜ì˜ ë§:
[root@computenode shared]# sbatch job.slurm
sbatch: error: Unable to open file job.slurm

ì´ ì—ëŸ¬ëŠ” Slurmì´ job.slurm íŒŒì¼ì„ í˜„ì¬ ë””ë ‰í„°ë¦¬ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤.

âœ… í™•ì¸ ë° í•´ê²° ë°©ë²•
1. í˜„ì¬ ìœ„ì¹˜ í™•ì¸

pwd
ls -l

2. ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ Head Nodeì—ì„œ ë§Œë“¤ì–´ì£¼ì„¸ìš”

Head Nodeì—ì„œ ë‹¤ìŒì²˜ëŸ¼ job.slurm íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”:

cd /home/mpiuser/shared
```
cat > job.slurm << 'EOF'
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=/home/mpiuser/shared/mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"

srun /home/mpiuser/shared/mpi_hello

echo "Job completed at $(date)"
EOF
```

ls -l /home/mpiuser/shared/mpi_hello
chmod +x /home/mpiuser/shared/mpi_hello

3. ë‹¤ì‹œ compute nodeì—ì„œ ì‹¤í–‰

cd /home/mpiuser/shared
sbatch job.slurm

ë‚˜ì˜ ë§:
[root@computenode shared]# ls -l
í•©ê³„ 20
-rwxr-xr-x. 1 root root 17728  5ì›” 16 15:50 mpi_hello

í˜„ì¬ ì´ íŒŒì¼ì˜ ì†Œìœ ìê°€ rootì´ê³ , ë§Œì•½ Slurm ì‘ì—…ì´ mpiuser ê°™ì€ ì¼ë°˜ ì‚¬ìš©ìë¡œ ì‹¤í–‰ëœë‹¤ë©´ ê¶Œí•œ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
âœ… í•´ê²° ìˆœì„œ
1. ê¶Œí•œ ë° ì†Œìœ ì ë³€ê²½ (head node ë˜ëŠ” compute nodeì—ì„œ)

chown mpiuser:mpiuser /home/mpiuser/shared/mpi_hello

ë˜ëŠ” ì „ì²´ ë””ë ‰í„°ë¦¬ì— ëŒ€í•´:

chown -R mpiuser:mpiuser /home/mpiuser/shared

2. Slurm ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ë„ /home/mpiuser/sharedì— ë§Œë“¤ê¸°

cd /home/mpiuser/shared

cat > job.slurm << 'EOF'
#!/bin/bash
#SBATCH --job-name=mpi_test
#SBATCH --nodes=2
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2
#SBATCH --output=/home/mpiuser/shared/mpi_job_%j.out

module load mpi/openmpi-x86_64

echo "Job started at $(date)"
echo "Running on $(hostname)"

srun /home/mpiuser/shared/mpi_hello

echo "Job completed at $(date)"
EOF

3. Slurm ì‘ì—… ì œì¶œ

cd /home/mpiuser/shared
sbatch job.slurm

4. ê²°ê³¼ í™•ì¸

cat mpi_job_*.out

ë‚˜ì˜ ë§:
[root@headnode ~]# pwd
/root
[root@headnode ~]# ls -l
í•©ê³„ 28
-rw-------. 1 root root 1383  5ì›” 13 15:44 anaconda-ks.cfg
-rw-r--r--. 1 root root   42  5ì›” 15 14:14 hostfile
-rw-r--r--. 1 root root  321  5ì›” 16 16:06 job.slurm
-rw-r--r--. 1 root root  542  5ì›” 15 13:48 mpi_hello.c
drwxr-xr-x. 2 root root   58  5ì›” 16 14:35 mpi_test
-rw-r--r--. 1 root root   12  1ì›” 28 09:17 nodes
-rwxr-xr-x. 1 root root   43  5ì›” 15 13:47 openmpi.sh
-rw-r--r--. 1 root root  155  5ì›” 15 15:58 test_job.sh
[root@headnode ~]# 
ChatGPTì˜ ë§:
ê³µìœ  ë””ë ‰í„°ë¦¬(/home/mpiuser/shared)ë¡œ ì´ë™ì´ í•„ìš”í•©ë‹ˆë‹¤.

âœ… í•´ê²° ì ˆì°¨
1. ê³µìœ  ë””ë ‰í„°ë¦¬ë¡œ íŒŒì¼ ë³µì‚¬ (headnodeì—ì„œ ì‹¤í–‰)

mkdir -p /home/mpiuser/shared
cp ~/mpi_hello /home/mpiuser/shared/
cp ~/job.slurm /home/mpiuser/shared/
chown mpiuser:mpiuser /home/mpiuser/shared/*
chmod +x /home/mpiuser/shared/mpi_hello

2. compute nodeì—ì„œ ê³µìœ  ë””ë ‰í„°ë¦¬ í™•ì¸

ls /home/mpiuser/shared

3. Slurm ì‘ì—… ì œì¶œ (head ë˜ëŠ” compute nodeì—ì„œ)

cd /home/mpiuser/shared
sbatch job.slurm

4. ì‘ì—… ì™„ë£Œ í›„ ê²°ê³¼ í™•ì¸

cat /home/mpiuser/shared/mpi_job_*.out

ë‚˜ì˜ ë§:
[root@headnode ~]# cp ~/mpi_hello /home/mpiuser/shared
cp: cannot stat '/root/mpi_hello': ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤

/root/mpi_hello íŒŒì¼ì„ mpi_hello.c íŒŒì¼ì„ ì»´íŒŒì¼í•˜ì—¬ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
âœ… í•´ê²° ì ˆì°¨
1. mpi_hello.c ì»´íŒŒì¼í•˜ê¸°

mpicc -o /root/mpi_hello /root/mpi_hello.c

2. ì‹¤í–‰ íŒŒì¼ì„ ê³µìœ  ë””ë ‰í„°ë¦¬ë¡œ ì´ë™

ì´ì œ ì»´íŒŒì¼ëœ mpi_hello íŒŒì¼ì„ /home/mpiuser/sharedë¡œ ë³µì‚¬í•©ë‹ˆë‹¤:

cp /root/mpi_hello /home/mpiuser/shared/
chown mpiuser:mpiuser /home/mpiuser/shared/mpi_hello
chmod +x /home/mpiuser/shared/mpi_hello

3. Slurm ì‘ì—… ì œì¶œ

ì´ì œ /home/mpiuser/shared/ ë””ë ‰í„°ë¦¬ì—ì„œ job.slurmì„ ì œì¶œí•©ë‹ˆë‹¤:

cd /home/mpiuser/shared
sbatch job.slurm

4. ê²°ê³¼ í™•ì¸

cat /home/mpiuser/shared/mpi_job_*.out

ë‚˜ì˜ ë§:
2. [root@headnode ~]# cp /root/mpi_hello /home/mpiuser/shared/
cp: cannot stat '/root/mpi_hello': ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤

ì´ëŠ” mpi_hello.c íŒŒì¼ì„ ì»´íŒŒì¼í•˜ì—¬ mpi_hello ì‹¤í–‰ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•Šì•„ì„œ ë°œìƒí•œ ë¬¸ì œì…ë‹ˆë‹¤.
âœ… í•´ê²° ë°©ë²•
1. mpi_hello.c íŒŒì¼ ì»´íŒŒì¼

mpicc -o /root/mpi_hello /root/mpi_hello.c

2. mpi_hello íŒŒì¼ì„ ê³µìœ  ë””ë ‰í„°ë¦¬ë¡œ ë³µì‚¬

cp /root/mpi_hello /home/mpiuser/shared/
chown mpiuser:mpiuser /home/mpiuser/shared/mpi_hello
chmod +x /home/mpiuser/shared/mpi_hello

3. Slurm ì‘ì—… ì œì¶œ

cd /home/mpiuser/shared
sbatch job.slurm

4. ì‘ì—… ì œì¶œ
cat /home/mpiuser/shared/mpi_job_*.out

ë‚˜ì˜ ë§:
[root@headnode ~]# mpicc -I /path/to/mpi/include -o my_program my_program.c
[root@headnode ~]# ls -l my_program
-rwxr-xr-x. 1 root root 17624  5ì›” 16 16:16 my_program
[root@headnode ~]# mpirun -np 4 ./my_program
--------------------------------------------------------------------------
mpirun has detected an attempt to run as root.

Running as root is *strongly* discouraged as any mistake (e.g., in
defining TMPDIR) or bug can result in catastrophic damage to the OS
file system, leaving your system in an unusable state.
--------------------------------------------------------------------------
ChatGPTì˜ ë§:

MPIëŠ” ë³´ì•ˆì„ ì´ìœ ë¡œ root ì‚¬ìš©ìë¡œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë©”ì‹œì§€ì—ì„œ ì„¤ëª…ëœ ëŒ€ë¡œ, mpirunì„ root ì‚¬ìš©ìë¡œ ì‹¤í–‰í•˜ë ¤ê³  í•  ë•Œ ê²½ê³ ê°€ ë°œìƒí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ë¥¼ ë¬´ì‹œí•˜ê³  ì‹¤í–‰í•˜ë ¤ë©´ --allow-run-as-root ì˜µì…˜ì„ ì¶”ê°€í•˜ê±°ë‚˜, í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
ë°©ë²• 1: --allow-run-as-root ì˜µì…˜ ì‚¬ìš©

ë‹¤ìŒê³¼ ê°™ì´ mpirun ëª…ë ¹ì–´ì— --allow-run-as-root ì˜µì…˜ì„ ì¶”ê°€í•˜ì—¬ rootë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

mpirun --allow-run-as-root -np 4 ./my_program

ë°©ë²• 2: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

export OMPI_ALLOW_RUN_AS_ROOT=1
export OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
mpirun -np 4 ./my_program

ìƒˆë¡œìš´ ì‚¬ìš©ì ìƒì„±: adduser mpiuser

í•´ë‹¹ ì‚¬ìš©ìë¡œ ë¡œê·¸ì¸: su - mpiuser

MPI í”„ë¡œê·¸ë¨ì„ ë‹¤ì‹œ ì‹¤í–‰: mpirun -np 4 ./my_program
